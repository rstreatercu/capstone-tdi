{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ea29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701beebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: 17\n",
      "Question 2: 120\n",
      "Question 3: 0.1294917449\n",
      "Question 4: 0.8121778351\n",
      "Question 5: 0.0008650662\n",
      "Question 6: 241.4857557857\n",
      "Question 7: 0.2436882547\n",
      "Question 8: 2.4097928147\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import datetime\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot\n",
    "from scipy import optimize\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "\n",
    "hotspots=pd.read_csv('NYC_Wi-Fi_Hotspot_Locations.csv')\n",
    "print(\"Question 1: {:d}\".format(len(hotspots[\"Provider\"].drop_duplicates())))\n",
    "\n",
    "bronx_hotspots = hotspots[hotspots[\"Borough Name\"]==\"Bronx\"]\n",
    "print(\"Question 2: {:d}\".format(bronx_hotspots[\"Provider\"].value_counts()[1]))\n",
    "\n",
    "# Convert to upper to ensure capitalization doesn't matter. >-1 will include NaN's and cases where \"park\" isn't found.\n",
    "# Ignore blanks in total number because we don't know if they are in parks or not.\n",
    "print(\"Question 3: {:.10f}\".format(float(len(hotspots[hotspots[\"Name\"].str.upper().str.find(\"PARK\")>-1]))/len(hotspots.dropna(subset=[\"Name\"]))))\n",
    "\n",
    "# Use \"not found\" or -1 comparison here- we cannot know the hotspot is in a library if there is no location type.\n",
    "not_library_hotspots = hotspots[hotspots[\"Location_T\"].str.upper().str.find(\"LIBRARY\")==-1]\n",
    "# It is free if remarks are blank, or just say the serial number. Otherwise, the remarks explain time/data/device limitations.\n",
    "n_free = len(not_library_hotspots[not_library_hotspots[\"Type\"].str.upper()==\"FREE\"])\n",
    "print(\"Question 4: {:.10f}\".format(float(n_free)/len(not_library_hotspots)))\n",
    "\n",
    "population = pd.read_csv(\"Census_Demographics_at_the_Neighborhood_Tabulation_Area__NTA__level.csv\")\n",
    "nta_counts = hotspots[\"Neighborhood Tabulation Area Code (NTACODE)\"].value_counts()\n",
    "nta_counts_trimmed = nta_counts[nta_counts>=30]\n",
    "hotspots_per_capita = []\n",
    "# Save hotspots per capita for each population value in which the NTA code matches one of the codes with at least 30 hotspots\n",
    "for ind in nta_counts_trimmed.index:\n",
    "    p = population[population[\"Geographic Area - Neighborhood Tabulation Area (NTA)* Code\"]==ind]\n",
    "    hotspots_per_capita.append(float(nta_counts_trimmed[ind])/p.loc[p.index[0]][\"Total Population 2010 Number\"])\n",
    "# Find n where length of list is 2n+1\n",
    "n=math.floor(len(hotspots_per_capita)/2)\n",
    "hotspots_per_capita.sort()\n",
    "Q1=statistics.median(hotspots_per_capita[:n])\n",
    "Q3=statistics.median(hotspots_per_capita[-n:])\n",
    "print(\"Question 5: {:.10f}\".format(Q3-Q1))\n",
    "\n",
    "# Start by eliminating hotspots at the same location- nearest three hotspots shouldn't include ones at the same\n",
    "# exact point where distance=0, and I'm interpreting the question as asking for the three nearest hotspot points,\n",
    "# so that if there were three hotspots in the same place at 100 feet away, they would only be counted once.\n",
    "# I'm also interpreting \"How far do you need to walk between hotspots\" as \"Don't weight multiple hotspots in one\n",
    "# location as more important than one hotspot\", which is why I'm dropping duplicates now instead of after finding\n",
    "# distances for each hotspot.\n",
    "hotspot_locs = hotspots.drop_duplicates(subset=\"Location (Lat, Long)\")\n",
    "avg_three_nearest = []\n",
    "for ind in hotspot_locs.index:\n",
    "    latitude=hotspot_locs[\"Latitude\"][ind]\n",
    "    longitude=hotspot_locs[\"Longitude\"][ind]\n",
    "    # delta phi and delta lambda from the wikipedia formula. latitudes are clearly given in degrees, so convert\n",
    "    # to radians.\n",
    "    dphi = (hotspot_locs[\"Latitude\"]-latitude)*np.pi/180.0\n",
    "    dlambda = (hotspot_locs[\"Longitude\"]-longitude)*np.pi/180.0\n",
    "    phi_m = (hotspot_locs[\"Latitude\"]+latitude)*(np.pi/180.0)/2.0\n",
    "    # Convert Earth's radius to feet\n",
    "    earth_radius = 6371*3280.84\n",
    "    D=earth_radius*np.sqrt(dphi**2 + (dlambda * np.cos(phi_m))**2)\n",
    "    # Sorting is less efficient than searching for mins, but only costs a second or two and is easier code to read\n",
    "    D.sort_values(inplace=True, ignore_index=True)\n",
    "    # Ignore first value because this is always zero (self)\n",
    "    avg_three_nearest.append(np.mean(D[1:3]))\n",
    "print(\"Question 6: {:.10f}\".format(np.median(avg_three_nearest)))\n",
    "\n",
    "# Start by eliminating dates that don't make sense\n",
    "valid_dates = pd.to_datetime(hotspots[\"Activated\"],errors=\"coerce\").dropna()\n",
    "print(\"Question 7: {:.10f}\".format(float(valid_dates.dt.weekday.value_counts().max())/len(valid_dates)))\n",
    "\n",
    "# Bin all months into the first day and count up values\n",
    "monthyear = pd.DataFrame(pd.to_datetime(valid_dates.dt.strftime(\"%m-%Y\")).value_counts().sort_index())\n",
    "monthyear_trimmed = monthyear[monthyear.index<pd.to_datetime(\"2018-07-01\")]\n",
    "# Get days from start by comparing difference between months. Binning by first day will weight each month by\n",
    "# its length relative to 30.5 days, which is correct because this will eliminate inaccuracies due to month length\n",
    "# variation.\n",
    "first_date=monthyear.index[0]\n",
    "x = ((monthyear_trimmed.index-first_date).days)/30.5\n",
    "y = np.array(monthyear_trimmed.Activated.to_list())\n",
    "# Polyfit seemed to be too sensitive to outliers, so I used a Huber regressor which is supposed to\n",
    "# be insensitive to outliers.\n",
    "huber = HuberRegressor(epsilon=1.0)\n",
    "huber.fit(np.expand_dims(x,1), np.expand_dims(y,1).ravel())\n",
    "print(\"Question 8: {:.10f}\".format(huber.coef_[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2737b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
